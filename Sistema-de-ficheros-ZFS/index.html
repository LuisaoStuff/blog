<!DOCTYPE html>
<html lang="pt-br">
    <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <title>Sistema de ficheros ZFS</title>
    <meta name="description" content="Administrador de Sistemas Informáticos en Red">

    <!-- Google Authorship Markup -->
    <link rel="author" href="https://plus.google.com/?rel=author">

    <!-- Social: Twitter -->
    <meta name="twitter:card" content="summary_large_image"> 
    <meta name="twitter:site" content="@">
    <meta name="twitter:title" content="Sistema de ficheros ZFS">
    <meta name="twitter:description" content="Administrador de Sistemas Informáticos en Red">
    
    <meta property="twitter:image:src" content="/assets/img/blog-image.png">
    
    <!-- Social: Facebook / Open Graph -->
    <meta property="og:url" content="/Sistema-de-ficheros-ZFS/">
    <meta property="og:title" content="Sistema de ficheros ZFS">
    
    <meta property="og:image" content="/assets/img/blog-image.png">
    
    <meta property="og:description" content="Administrador de Sistemas Informáticos en Red">
    <meta property="og:site_name" content="Luis Vázquez Alejo">
    <!-- Social: Google+ / Schema.org  -->
    <meta itemprop="name" content="Sistema de ficheros ZFS"/>
    <meta itemprop="description" content="Administrador de Sistemas Informáticos en Red">
    <meta itemprop="image" content="/assets/img/blog-image.png"/>
    <!-- Favicon -->
    <link rel="shortcut icon" href="/assets/img/icons/favicon.ico" type="image/x-icon" />
    <!-- Apple Touch Icons -->
    <link rel="apple-touch-icon" href="/assets/img/icons/apple-touch-icon.png" />
    <link rel="apple-touch-icon" sizes="57x57" href="/assets/img/icons/apple-touch-icon-57x57.png" />
    <link rel="apple-touch-icon" sizes="72x72" href="/assets/img/icons/apple-touch-icon-72x72.png" />
    <link rel="apple-touch-icon" sizes="114x114" href="/assets/img/icons/apple-touch-icon-114x114.png" />
    <link rel="apple-touch-icon" sizes="144x144" href="/assets/img/icons/apple-touch-icon-144x144.png" />
    <link rel="apple-touch-icon" sizes="60x60" href="/assets/img/icons/apple-touch-icon-60x60.png" />
    <link rel="apple-touch-icon" sizes="120x120" href="/assets/img/icons/apple-touch-icon-120x120.png" />
    <link rel="apple-touch-icon" sizes="76x76" href="/assets/img/icons/apple-touch-icon-76x76.png" />
    <link rel="apple-touch-icon" sizes="152x152" href="/assets/img/icons/apple-touch-icon-152x152.png" />
    <!-- Windows 8 Tile Icons -->
    <meta name="application-name" content="Luis Vázquez Alejo Blog">
    <meta name="msapplication-TileColor" content="#0562DC">
    <meta name="msapplication-square70x70logo" content="smalltile.png" />
    <meta name="msapplication-square150x150logo" content="mediumtile.png" />
    <meta name="msapplication-wide310x150logo" content="widetile.png" />
    <meta name="msapplication-square310x310logo" content="largetile.png" />
    <!-- Android Lolipop Theme Color -->
    <meta name="theme-color" content="#0562DC">

    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="/Sistema-de-ficheros-ZFS/">
    <link rel="alternate" type="application/rss+xml" title="Luis Vázquez Alejo" href="/feed.xml" />
</head>

    <body>
        <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" display="none" version="1.1"><defs><symbol id="icon-menu" viewBox="0 0 1024 1024"><path class="path1" d="M128 213.333h768q17.667 0 30.167 12.5t12.5 30.167-12.5 30.167-30.167 12.5h-768q-17.667 0-30.167-12.5t-12.5-30.167 12.5-30.167 30.167-12.5zM128 725.333h768q17.667 0 30.167 12.5t12.5 30.167-12.5 30.167-30.167 12.5h-768q-17.667 0-30.167-12.5t-12.5-30.167 12.5-30.167 30.167-12.5zM128 469.333h768q17.667 0 30.167 12.5t12.5 30.167-12.5 30.167-30.167 12.5h-768q-17.667 0-30.167-12.5t-12.5-30.167 12.5-30.167 30.167-12.5z"/></symbol><symbol id="icon-search" viewBox="0 0 951 1024"><path class="path1" d="M658.286 475.429q0-105.714-75.143-180.857t-180.857-75.143-180.857 75.143-75.143 180.857 75.143 180.857 180.857 75.143 180.857-75.143 75.143-180.857zM950.857 950.857q0 29.714-21.714 51.429t-51.429 21.714q-30.857 0-51.429-21.714l-196-195.429q-102.286 70.857-228 70.857-81.714 0-156.286-31.714t-128.571-85.714-85.714-128.571-31.714-156.286 31.714-156.286 85.714-128.571 128.571-85.714 156.286-31.714 156.286 31.714 128.571 85.714 85.714 128.571 31.714 156.286q0 125.714-70.857 228l196 196q21.143 21.143 21.143 51.429z"/></symbol><symbol id="icon-email" viewBox="0 0 1024 1024"><path class="path1" d="M950.857 859.429v-438.857q-18.286 20.571-39.429 37.714-153.143 117.714-243.429 193.143-29.143 24.571-47.429 38.286t-49.429 27.714-58.571 14h-1.143q-27.429 0-58.571-14t-49.429-27.714-47.429-38.286q-90.286-75.429-243.429-193.143-21.143-17.143-39.429-37.714v438.857q0 7.429 5.429 12.857t12.857 5.429h841.143q7.429 0 12.857-5.429t5.429-12.857zM950.857 258.857v-14t-0.286-7.429-1.714-7.143-3.143-5.143-5.143-4.286-8-1.429h-841.143q-7.429 0-12.857 5.429t-5.429 12.857q0 96 84 162.286 110.286 86.857 229.143 181.143 3.429 2.857 20 16.857t26.286 21.429 25.429 18 28.857 15.714 24.571 5.143h1.143q11.429 0 24.571-5.143t28.857-15.714 25.429-18 26.286-21.429 20-16.857q118.857-94.286 229.143-181.143 30.857-24.571 57.429-66t26.571-75.143zM1024 237.714v621.714q0 37.714-26.857 64.571t-64.571 26.857h-841.143q-37.714 0-64.571-26.857t-26.857-64.571v-621.714q0-37.714 26.857-64.571t64.571-26.857h841.143q37.714 0 64.571 26.857t26.857 64.571z"/></symbol><symbol id="icon-close" viewBox="0 0 805 1024"><path class="path1" d="M741.714 755.429q0 22.857-16 38.857l-77.714 77.714q-16 16-38.857 16t-38.857-16l-168-168-168 168q-16 16-38.857 16t-38.857-16l-77.714-77.714q-16-16-16-38.857t16-38.857l168-168-168-168q-16-16-16-38.857t16-38.857l77.714-77.714q16-16 38.857-16t38.857 16l168 168 168-168q16-16 38.857-16t38.857 16l77.714 77.714q16 16 16 38.857t-16 38.857l-168 168 168 168q16 16 16 38.857z"/></symbol><symbol id="icon-twitter" viewBox="0 0 951 1024"><path class="path1" d="M925.714 233.143q-38.286 56-92.571 95.429 0.571 8 0.571 24 0 74.286-21.714 148.286t-66 142-105.429 120.286-147.429 83.429-184.571 31.143q-154.857 0-283.429-82.857 20 2.286 44.571 2.286 128.571 0 229.143-78.857-60-1.143-107.429-36.857t-65.143-91.143q18.857 2.857 34.857 2.857 24.571 0 48.571-6.286-64-13.143-106-63.714t-42-117.429v-2.286q38.857 21.714 83.429 23.429-37.714-25.143-60-65.714t-22.286-88q0-50.286 25.143-93.143 69.143 85.143 168.286 136.286t212.286 56.857q-4.571-21.714-4.571-42.286 0-76.571 54-130.571t130.571-54q80 0 134.857 58.286 62.286-12 117.143-44.571-21.143 65.714-81.143 101.714 53.143-5.714 106.286-28.571z"/></symbol><symbol id="icon-facebook" viewBox="0 0 585 1024"><path class="path1" d="M548 6.857v150.857h-89.714q-49.143 0-66.286 20.571t-17.143 61.714v108h167.429l-22.286 169.143h-145.143v433.714h-174.857v-433.714h-145.714v-169.143h145.714v-124.571q0-106.286 59.429-164.857t158.286-58.571q84 0 130.286 6.857z"/></symbol><symbol id="icon-rss" viewBox="0 0 805 1024"><path class="path1" d="M219.429 768q0 45.714-32 77.714t-77.714 32-77.714-32-32-77.714 32-77.714 77.714-32 77.714 32 32 77.714zM512 838.286q1.143 16-9.714 27.429-10.286 12-26.857 12h-77.143q-14.286 0-24.571-9.429t-11.429-23.714q-12.571-130.857-105.429-223.714t-223.714-105.429q-14.286-1.143-23.714-11.429t-9.429-24.571v-77.143q0-16.571 12-26.857 9.714-9.714 24.571-9.714h2.857q91.429 7.429 174.857 46t148 103.714q65.143 64.571 103.714 148t46 174.857zM804.571 839.429q1.143 15.429-10.286 26.857-10.286 11.429-26.286 11.429h-81.714q-14.857 0-25.429-10t-11.143-24.286q-6.857-122.857-57.714-233.429t-132.286-192-192-132.286-233.429-58.286q-14.286-0.571-24.286-11.143t-10-24.857v-81.714q0-16 11.429-26.286 10.286-10.286 25.143-10.286h1.714q149.714 7.429 286.571 68.571t243.143 168q106.857 106.286 168 243.143t68.571 286.571z"/></symbol><symbol id="icon-google-plus" viewBox="0 0 951 1024"><path class="path1" d="M420 454.857q0 20.571 18.286 40.286t44.286 38.857 51.714 42 44 59.429 18.286 81.143q0 51.429-27.429 98.857-41.143 69.714-120.571 102.571t-170.286 32.857q-75.429 0-140.857-23.714t-98-78.571q-21.143-34.286-21.143-74.857 0-46.286 25.429-85.714t67.714-65.714q74.857-46.857 230.857-57.143-18.286-24-27.143-42.286t-8.857-41.714q0-20.571 12-48.571-26.286 2.286-38.857 2.286-84.571 0-142.571-55.143t-58-139.714q0-46.857 20.571-90.857t56.571-74.857q44-37.714 104.286-56t124.286-18.286h238.857l-78.857 50.286h-74.857q42.286 36 64 76t21.714 91.429q0 41.143-14 74t-33.714 53.143-39.714 37.143-34 35.143-14 37.714zM336.571 400q21.714 0 44.571-9.429t37.714-24.857q30.286-32.571 30.286-90.857 0-33.143-9.714-71.429t-27.714-74-48.286-59.143-66.857-23.429q-24 0-47.143 11.143t-37.429 30q-26.857 33.714-26.857 91.429 0 26.286 5.714 55.714t18 58.857 29.714 52.857 42.857 38.286 55.143 14.857zM337.714 898.857q33.143 0 63.714-7.429t56.571-22.286 41.714-41.714 15.714-62.286q0-14.286-4-28t-8.286-24-15.429-23.714-16.857-20-22-19.714-20.857-16.571-23.714-17.143-20.857-14.857q-9.143-1.143-27.429-1.143-30.286 0-60 4t-61.429 14.286-55.429 26.286-39.143 42.571-15.429 60.286q0 40 20 70.571t52.286 47.429 68 25.143 72.857 8.286zM800.571 398.286h121.714v61.714h-121.714v125.143h-60v-125.143h-121.143v-61.714h121.143v-124h60v124z"/></symbol><symbol id="icon-angle-down" viewBox="0 0 658 1024"><path class="path1" d="M614.286 420.571q0 7.429-5.714 13.143l-266.286 266.286q-5.714 5.714-13.143 5.714t-13.143-5.714l-266.286-266.286q-5.714-5.714-5.714-13.143t5.714-13.143l28.571-28.571q5.714-5.714 13.143-5.714t13.143 5.714l224.571 224.571 224.571-224.571q5.714-5.714 13.143-5.714t13.143 5.714l28.571 28.571q5.714 5.714 5.714 13.143z"/></symbol><symbol id="icon-github-alt" viewBox="0 0 951 1024"><path class="path1" d="M365.714 694.857q0 22.857-7.143 46.857t-24.571 43.429-41.429 19.429-41.429-19.429-24.571-43.429-7.143-46.857 7.143-46.857 24.571-43.429 41.429-19.429 41.429 19.429 24.571 43.429 7.143 46.857zM731.429 694.857q0 22.857-7.143 46.857t-24.571 43.429-41.429 19.429-41.429-19.429-24.571-43.429-7.143-46.857 7.143-46.857 24.571-43.429 41.429-19.429 41.429 19.429 24.571 43.429 7.143 46.857zM822.857 694.857q0-68.571-39.429-116.571t-106.857-48q-23.429 0-111.429 12-40.571 6.286-89.714 6.286t-89.714-6.286q-86.857-12-111.429-12-67.429 0-106.857 48t-39.429 116.571q0 50.286 18.286 87.714t46.286 58.857 69.714 34.286 80 16.857 85.143 4h96q46.857 0 85.143-4t80-16.857 69.714-34.286 46.286-58.857 18.286-87.714zM950.857 594.286q0 118.286-34.857 189.143-21.714 44-60.286 76t-80.571 49.143-97.143 27.143-98 12.571-95.429 2.571q-44.571 0-81.143-1.714t-84.286-7.143-87.143-17.143-78.286-29.429-69.143-46.286-49.143-65.714q-35.429-70.286-35.429-189.143 0-135.429 77.714-226.286-15.429-46.857-15.429-97.143 0-66.286 29.143-124.571 61.714 0 108.571 22.571t108 70.571q84-20 176.571-20 84.571 0 160 18.286 60-46.857 106.857-69.143t108-22.286q29.143 58.286 29.143 124.571 0 49.714-15.429 96 77.714 91.429 77.714 227.429z"/></symbol></defs></svg>

        <header class="header-post" role="banner">
    <div class="content">
        
            <time itemprop="datePublished" datetime="2020-01-25 10:59:00 +0000" class="date">25 Jan 2020</time>
        
        <h1 class="post-title" itemprop="name">Sistema de ficheros ZFS</h1>
        <p itemprop="description" class="subtitle"></p>
    </div>
     <a class="down" data-scroll href="#scroll"><svg class="icon icon-angle-down"><use xlink:href="#icon-angle-down"></use></svg></a>
     <div class="search-wrapper">
    <div class="search-form">
        <input type="text" class="search-field" placeholder="Buscar...">
        <svg class="icon-remove-sign"><use xlink:href="#icon-close"></use></svg>
        <ul class="search-results search-list"></ul>
    </div>
</div>

<div id="fade" class="overlay"></div>
<a id="slide" class="slideButton fade">
    <svg id="open" class="icon-menu"><use xlink:href="#icon-menu"></use></svg>
    <svg id="close" class="icon-menu"><use xlink:href="#icon-close"></use></svg>
</a>
<aside id="sidebar">
<nav id="navigation">
  <h2>MENU</h2>
  <ul>
    
    
      <li><a href="/">Inicio</a></li>
    
    
    
      <li><a href="/categorias">Categorías</a></li>
    
    
    
      <li><a href="/tags">Tags</a></li>
    
    
    
      <li><a href="/sobre-mi">Sobre mí</a></li>
    
    
  </ul>
</nav>
</aside>
<a id="search" class="dosearch">
    <svg class="icon-menu icon-search"><use xlink:href="#icon-search"></use></svg>
</a>

</header>

        <section class="post" itemscope itemtype="http://schema.org/BlogPosting">

            <article role="article" id="scroll" class="post-content" itemprop="articleBody">
                <h2 id="introducción">Introducción</h2>

<p><strong>ZFS</strong> es un sistema de ficheros que cuenta con una serie de características avanzadas, tales como <em>auto-reparación</em> o <em>copy-on-write</em>. No obstante, aún siendo extremádamente útil en el entorno de servidores, <strong>ZFS</strong> no está tan extendido como cabría esperar. Su licencia <strong>CDDL</strong> (incompatible con la licencia <strong>GPL</strong> de Linux) y diversos acontecimientos durante su desarrollo pusieron bastantes trabas en la implementación de este sistema de ficheros en el <em>Kernel Linux</em>.
Dicho esto vamos a simular un escenario con <em>Vagrant</em> donde tendremos una máquina a la que le agregaremos varios discos, para posteriormente, instalar <strong>ZFS</strong> y “jugar” un poco con él.</p>

<h2 id="preparación-del-escenario">Preparación del escenario</h2>

<p>Si queréis probarlo, os dejo el <a href="/docs/EscenarioZFS/Vagrantfile">Vagrantfile</a> donde básicamente creamos una máquina <em>Debian Buster</em> con 5 discos adicionales de 400Mb cada uno, y añadimos los repositorios de <em>back-ports</em> de Debian. Estos repositorios nos harán falta a la hora de instalar el paquete, ya que al no tener una licencia <strong>GPL</strong>, el propio <em>Debian</em> no puede incluirlo en la rama <em>main</em> y mucho menos en los paquetes de la instalación.
Una vez creada la máquina, actualizamos el sistema e instalamos el paquete <code class="language-plaintext highlighter-rouge">zfsutils-linux</code>. Pero antes debemos instalar los <code class="language-plaintext highlighter-rouge">linux-headers</code>, que en mi caso corresponde con la versión del <em>kernel</em> <strong>4.19.0.6</strong>.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">apt upgrade <span class="nt">-y</span>
apt <span class="nb">install</span> <span class="nt">-y</span> linux-headers-4.19.0-6-amd64
apt <span class="nb">install</span> <span class="nt">-yt</span> buster-backports dkms spl-dkms
apt <span class="nb">install</span> <span class="nt">-yt</span> buster-backports zfs-dkms zfsutils-linux</code></pre></figure>

<p>Para comprobar que <strong>zfs</strong> está funcionando podemos ejecutar:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">root@zfsMachine:~# systemctl status zfs-mount
● zfs-mount.service - Mount ZFS filesystems
   Loaded: loaded <span class="o">(</span>/lib/systemd/system/zfs-mount.service<span class="p">;</span> enabled<span class="p">;</span> vendor preset: enabled<span class="o">)</span>
   Active: active <span class="o">(</span>exited<span class="o">)</span> since Tue 2020-01-21 21:20:39 CET<span class="p">;</span> 19min ago
     Docs: man:zfs<span class="o">(</span>8<span class="o">)</span>
 Main PID: 6631 <span class="o">(</span><span class="nv">code</span><span class="o">=</span>exited, <span class="nv">status</span><span class="o">=</span>0/SUCCESS<span class="o">)</span>
    Tasks: 0 <span class="o">(</span>limit: 1150<span class="o">)</span>
   Memory: 0B
   CGroup: /system.slice/zfs-mount.service

ene 21 21:20:39 zfsMachine systemd[1]: Starting Mount ZFS filesystems...
ene 21 21:20:39 zfsMachine systemd[1]: Started Mount ZFS filesystems.</code></pre></figure>

<h2 id="creación-y-configuración-de-los-pool">Creación y configuración de los “pool”</h2>

<p>Un <em>pool</em> en <strong>ZFS</strong> sería algo así lo que un grupo de volúmenes en <strong>LVM</strong>. La principal diferencia es que con <strong>ZFS</strong> fusionamos lo que es la capa de volúmenes lógicos y la capa de <em>RAID</em> software. Es por esto que durante la creación y configuración de un <em>pool</em>, podemos definir el tipo de <em>RAID</em> que vamos a crear, así como otras diversas opciones; discos caché, tamaño de sectores, etc.
Los tipos de <em>RAID</em> soportados por <strong>ZFS</strong> son los siguientes.</p>

<ul>
  <li><a href="https://es.wikipedia.org/wiki/RAID#RAID_0_(Data_Striping,_Striped_Volume)">RAID0</a>. Es igual que el tradicional. Mejora velocidad lectura/escritura pero <em>no ofrece redundancia</em>.</li>
  <li><a href="https://es.wikipedia.org/wiki/RAID#RAID_1_(espejo)">RAID1</a>. Es igual que el tradicional. Ofrece una redundancia igual a <strong>n-1</strong></li>
  <li><a href="https://es.wikipedia.org/wiki/RAID#RAID_1+0">RAID10</a>. No hay diferencia con el tradicional. Es la combinación del <em>RAID</em> 1 y 0. Es decir, es la agrupación de <em>RAID 1</em> en <em>RAID 0</em>.</li>
  <li><a href="https://es.wikipedia.org/wiki/RAID#RAID_Z">RAIDZ</a> <strong>1</strong>, <strong>2</strong> y <strong>3</strong>. Dependiendo del tipo tiene uno, dos o tres bit de paridad, necesitándose 3, 4 o 5 discos respectivamente.</li>
</ul>

<p>En esta ocasión vamos a trabar con <strong>RAIDZ-2</strong> por lo que necesitaremos <strong>4</strong> discos. La creación de estos <em>pool</em> se realiza con el comando <code class="language-plaintext highlighter-rouge">zpool</code>.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">zpool create <span class="nt">-f</span> EjemploRaidZ raidz2 /dev/sdb /dev/sdc /dev/sdd /dev/sde
zpool status
  pool: EjemploRaidZ
 state: ONLINE
  scan: none requested
config:

	NAME          STATE     READ WRITE CKSUM
	EjemploRaidZ  ONLINE       0     0     0
	  raidz2-0    ONLINE       0     0     0
	    sdb       ONLINE       0     0     0
	    sdc       ONLINE       0     0     0
	    sdd       ONLINE       0     0     0
	    sde       ONLINE       0     0     0

errors: No known data errors</code></pre></figure>

<p>De la misma forma que en los <em>RAID</em> tradicionales (como cuando los gestionamos con <code class="language-plaintext highlighter-rouge">mdadm</code>), podemos añadir discos de reserva (<em>hot spare</em>). De esta manera, si uno de los discos falla, el <em>hot spare</em> se reemplazaría de forma automática. Para añadir un disco en modo reserva ejecutamos:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">zpool add EjemploRaidZ spare sdf
zpool status
  pool: EjemploRaidZ
 state: ONLINE
  scan: none requested
config:

	NAME          STATE     READ WRITE CKSUM
	EjemploRaidZ  ONLINE       0     0     0
	  raidz2-0    ONLINE       0     0     0
	    sdb       ONLINE       0     0     0
	    sdc       ONLINE       0     0     0
	    sdd       ONLINE       0     0     0
	    sde       ONLINE       0     0     0
	spares
	  sdf         AVAIL

errors: No known data errors</code></pre></figure>

<h2 id="simulación-de-fallos-y-pruebas-de-redundancia">Simulación de fallos y pruebas de redundancia</h2>

<p>Vamos a ejecutar una serie de comandos para simular un fallo de un disco con el parámetro <code class="language-plaintext highlighter-rouge">offline</code> y vamos a obsevar como se reemplaza automáticamente y sin perder información. Antes vamos a crear varios ficheros para comprobar que siguen manteniendo su integridad. 
Aunque a priori la gestión de <strong>zfs</strong> pueda parecerse en gran medida a la gestión de <em>RAID</em> con <strong>mdadm</strong>, la verdad es que conceptualmente no tienen nada que ver. Ya de primeras cuando gestionamos un <em>RAID</em> software con <strong>mdadm</strong>, necesitamos (tras haber particionado, aplicado una capa de <em>LVM</em>, o no) montar el dispositivo de bloques que se nos ha generado. En contraposición, el <em>pool</em> que se nos genera con <strong>zfs</strong> se monta automáticamete en la raiz del sistema con el nombre que le hayamos asignado. No obstante, también podríamos generar volúmenes y posteriormente darles el formato que queramos (con la ventaja de que seguiría funcionando por debajo con <strong>zfs</strong>). Aunque esto último lo veremos en el siguiente apartado.
Por el momento vamos a generar una serie de ficheros en <strong>/EjemploRaidZ</strong>.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="c">#Creamos un arbol de directorios</span>
<span class="nb">mkdir</span> <span class="nt">-p</span> /EjemploRaidZ/directorio/subdirectorio/
<span class="c">#Creamos ficheros aleatorios</span>
<span class="nb">dd </span><span class="k">if</span><span class="o">=</span>/dev/urandom <span class="nv">of</span><span class="o">=</span>/EjemploRaidZ/ficheroRandom <span class="nv">bs</span><span class="o">=</span>64K <span class="nv">count</span><span class="o">=</span>300
<span class="nb">dd </span><span class="k">if</span><span class="o">=</span>/dev/urandom <span class="nv">of</span><span class="o">=</span>/EjemploRaidZ/directorio/ficheroRandom <span class="nv">bs</span><span class="o">=</span>64K <span class="nv">count</span><span class="o">=</span>600
<span class="nb">echo</span> <span class="s2">"Prueba de un fichero"</span> <span class="o">&gt;</span> /EjemploRaidZ/directorio/subdirectorio/fichtest</code></pre></figure>

<p>Antes de hacer fallar uno de los discos, vamos a generar un <em>checksum</em> de los dos ficheros aleatorios con el comando <code class="language-plaintext highlighter-rouge">md5sum</code>. De esta manera si cambia aunque sea un <em>byte</em> de alguno de los dos ficheros, el <em>checksum</em> será completamente distinto.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nb">md5sum </span>ficheroRandom 
935a8851f013f41e0dab7afad20b7377  ficheroRandom
<span class="nb">md5sum </span>directorio/ficheroRandom 
49b5591a6b7daecbecd185bae9e6f769  directorio/ficheroRandom</code></pre></figure>

<p>Para simular el fallo del disco, vamos a ejecutar el siguiente comando.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">zpool offline <span class="nt">-f</span> EjemploRaidZ sdc
zpool status
  pool: EjemploRaidZ
 state: DEGRADED
status: One or more devices are faulted <span class="k">in </span>response to persistent errors.
	Sufficient replicas exist <span class="k">for </span>the pool to <span class="k">continue </span>functioning <span class="k">in </span>a
	degraded state.
action: Replace the faulted device, or use <span class="s1">'zpool clear'</span> to mark the device
	repaired.
  scan: none requested
config:

	NAME          STATE     READ WRITE CKSUM
	EjemploRaidZ  DEGRADED     0     0     0
	  raidz2-0    DEGRADED     0     0     0
	    sdb       ONLINE       0     0     0
	    sdc       FAULTED      0     0     0  external device fault
	    sdd       ONLINE       0     0     0
	    sde       ONLINE       0     0     0
	spares
	  sdf         AVAIL

errors: No known data errors

<span class="nb">md5sum </span>ficheroRandom 
935a8851f013f41e0dab7afad20b7377  ficheroRandom
<span class="nb">md5sum </span>directorio/ficheroRandom 
49b5591a6b7daecbecd185bae9e6f769  directorio/ficheroRandom</code></pre></figure>

<p>Como podemos comprobar, aunque el <em>RAID</em> esté degradado, gracias a que tiene una tolerancia a fallos (gracias a los 2 <em>bits de paridad</em> repartidos por todos los discos), seguimos obteniendo los mismos <em>checksum</em> por lo tanto la <strong>integridad</strong> de los ficheros está <strong>garantizada</strong>. No obstante ahora mismo nos encontramos sin redundancia, por lo que si quisiésemos acoplar el disco de reserva solo tenemos que ejecutar:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">zpool replace <span class="nt">-f</span> EjemploRaidZ sdc sdf
zpool status
  pool: EjemploRaidZ
 state: DEGRADED
status: One or more devices are faulted <span class="k">in </span>response to persistent errors.
	Sufficient replicas exist <span class="k">for </span>the pool to <span class="k">continue </span>functioning <span class="k">in </span>a
	degraded state.
action: Replace the faulted device, or use <span class="s1">'zpool clear'</span> to mark the device
	repaired.
  scan: resilvered 28.7M <span class="k">in </span>0 days 00:00:00 with 0 errors on Sun Jan 26 17:27:07 2020
config:

	NAME          STATE     READ WRITE CKSUM
	EjemploRaidZ  DEGRADED     0     0     0
	  raidz2-0    DEGRADED     0     0     0
	    sdb       ONLINE       0     0     0
	    spare-1   DEGRADED     0     0     0
	      sdc     FAULTED      0     0     0  external device fault
	      sdf     ONLINE       0     0     0
	    sdd       ONLINE       0     0     0
	    sde       ONLINE       0     0     0
	spares
	  sdf         INUSE     currently <span class="k">in </span>use

errors: No known data errors</code></pre></figure>

<p>Ahora tenemos dos opciones; podemos indicar que hemos reparado el disco que nos ha fallado, o podemos eliminarlo del <em>RAID</em> y dejar funcionando el disco que antes teníamos de reserva. Si optamos por la primera opción, tal y como nos indica la propia salida del estado del <em>zpool</em>, ejecutamos <code class="language-plaintext highlighter-rouge">zpool clear EjemploRaidZ sdc</code> para indicar que ya hemos reparado el disco.
Si por el contrario, el disco que nos ha fallado no podemos repararlo, el método correcto sería eliminarlo del <em>RAID</em> (dejar trabajando al antiguo disco de reserva) y añadir un nuevo disco como reserva. En este segundo escenario ejecutaríamos <code class="language-plaintext highlighter-rouge">zpool detach EjemploRaidZ sdc</code> para extraerlo del <em>RAID</em> y después añadir otro disco como reserva, tal y como hicimos antes.</p>

<h3 id="restauración-de-un-raid-con-rollback">Restauración de un RAID con “rollback”</h3>

<p><strong>ZFS</strong> tiene una funcionalidad parecida al <em>rollback</em> de las bases de datos de <em>Oracle</em>, y es que en un determinado momento en el que hemos tenido un número de fallos mayor al nivel de redundancia que teníamos disponible, podemos volver a un estado de este mismo <em>RAID</em>, anteior a dichos fallos.
Para poder hacer esto primero tendremos que guardar una <em>snapshot</em>, que será nuestro “punto de guardado” al que podremos volver posteriormente. Para crearlo tan solo ejecutamos:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">zpool checkpoint EjemploRaidZ</code></pre></figure>

<p>Vamos a generar los <em>checksum</em> igual que antes para comprobar la integridad de los archivos una vez que restauremos el <strong>checkpoint</strong>.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nb">md5sum </span>directorio/ficheroRandom 
49b5591a6b7daecbecd185bae9e6f769  directorio/ficheroRandom
<span class="nb">md5sum </span>ficheroRandom 
935a8851f013f41e0dab7afad20b7377  ficheroRandom</code></pre></figure>

<p>Ahora establecemos como <strong>FAULTED</strong> dos de los discos del <em>RAID</em>.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">zpool offline <span class="nt">-f</span> EjemploRaidZ sdc
zpool offline <span class="nt">-f</span> EjemploRaidZ sde
zpool status
  pool: EjemploRaidZ
 state: DEGRADED
status: One or more devices are faulted <span class="k">in </span>response to persistent errors.
	Sufficient replicas exist <span class="k">for </span>the pool to <span class="k">continue </span>functioning <span class="k">in </span>a
	degraded state.
action: Replace the faulted device, or use <span class="s1">'zpool clear'</span> to mark the device
	repaired.
  scan: resilvered 28.6M <span class="k">in </span>0 days 00:00:00 with 0 errors on Sun Jan 26 17:37:59 2020
checkpoint: created Sun Jan 26 17:49:02 2020, consumes 482K
config:

	NAME          STATE     READ WRITE CKSUM
	EjemploRaidZ  DEGRADED     0     0     0
	  raidz2-0    DEGRADED     0     0     0
	    sdb       ONLINE       0     0     0
	    sdf       FAULTED      0     0     0  external device fault
	    sdd       ONLINE       0     0     0
	    sde       FAULTED      0     0     0  external device fault

errors: No known data errors</code></pre></figure>

<p>Para restaurar el estado del <em>RAID</em>, lo exportamos e importamos utilizando el parámtero <code class="language-plaintext highlighter-rouge">--rewind-to-checkpoint</code>.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">zpool <span class="nb">export </span>EjemploRaidZ
zpool import <span class="nt">--rewind-to-checkpoint</span> EjemploRaidZ
zpool status
  pool: EjemploRaidZ
 state: ONLINE
  scan: none requested
config:

	NAME          STATE     READ WRITE CKSUM
	EjemploRaidZ  ONLINE       0     0     0
	  raidz2-0    ONLINE       0     0     0
	    sdb       ONLINE       0     0     0
	    sdf       ONLINE       0     0     0
	    sdd       ONLINE       0     0     0
	    sde       ONLINE       0     0     0</code></pre></figure>

<p>Y comprobamos los <em>checksum</em></p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nb">md5sum </span>ficheroRandom 
935a8851f013f41e0dab7afad20b7377  ficheroRandom
<span class="nb">md5sum </span>directorio/ficheroRandom 
49b5591a6b7daecbecd185bae9e6f769  directorio/ficheroRandom</code></pre></figure>

<p>Como podemos ver, todo está correcto!</p>

<h3 id="gestión-eficiente-del-almacenamiento">Gestión eficiente del almacenamiento</h3>

<h4 id="copy-on-write-cow">Copy on write [COW]</h4>

<p>Básicamente <strong>COW</strong> es una funcionalidad cuyo fin es el ahorro de espacio (tanto en capacidad como en inodos) a través de un tipo de <em>copia diferencial</em>. Para hacer uso de esto, tenemos que especificarlo con el parámetro específico de cada paquete. Por ejemplo, en el caso del comando <strong>cp</strong>, tendríamos que utilizar <code class="language-plaintext highlighter-rouge">reflink=always</code>. De esta forma, se crearía un fichero que funcionaría como un puntero al original e iría aumentando de tamaño conforme se fuese diferenciando del original.
No obstante, si nos encontramos en la versión de <strong>OpenZFS</strong>, empaquetada para <em>Debian Buster</em>, a día de hoy, esta funcionalidad no está completamente soportada.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">root@zfsMachine:/EjemploRaidZ# <span class="nb">cp</span> <span class="nt">--reflink</span><span class="o">=</span>always ficheroRandom ficheroRandom2
<span class="nb">cp</span>: failed to clone <span class="s1">'ficheroRandom2'</span> from <span class="s1">'ficheroRandom'</span>: Operation not supported</code></pre></figure>

<h4 id="deduplicación">Deduplicación</h4>

<p>En esencia es igual que el COW, solo que cuando lo activamos, no nos haría falta especificarlo como hicimos antes. Para activarlo tan solo ejecutamos:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="c"># Sintaxis</span>
<span class="c"># zfs set dedup=on|verify zfspool[/vol]</span>
zfs <span class="nb">set </span><span class="nv">dedup</span><span class="o">=</span>on EjemploRaidZ</code></pre></figure>

<h3 id="creación-de-volúmenes-lógicos">Creación de volúmenes lógicos</h3>

<p>Esta funcionalidad es de gran utilidad ya que podríamos utilizar <em>ZFS</em> como base de una <a href="https://es.wikipedia.org/wiki/Red_de_%C3%A1rea_de_almacenamiento">SAN</a>, gracias a que estos volúmenes se comportan como cualquier dispositivo de bloques a ojos del sistema operativo. Podríamos particionarlos, darles un formato, etc, y todo esto funcionando por debajo <em>ZFS</em>, incluyendo todas las ventajas que hemos mencionado antes!
Para crear estos dispositivos de bloques necesitamos tener un <em>zpool</em>, como el que hicimos antes. La sintaxis básica sería la siguiente:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">zfs create <span class="nt">-V</span> <span class="o">[</span>tamaño] <span class="o">[</span>nombre-del-pool]/[nombre-del-volumen]</code></pre></figure>

<p>En mi caso sería:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">zfs create <span class="nt">-V</span> 200mb EjemploRaidZ/vol1
zfs list
NAME                USED  AVAIL     REFER  MOUNTPOINT
EjemploRaidZ        209M   428M     35.9K  /EjemploRaidZ
EjemploRaidZ/vol1   208M   637M     17.9K  -</code></pre></figure>

<p>Para poder darle algún tipo de uso dentro del sistema, tendríamos que darle un formato reconocible, en este caso lo formatearemos con <strong>ext4</strong> y lo montaremos en <strong>/home/vagrant/vol1</strong>:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">mkfs.ext4 /dev/zvol/EjemploRaidZ/vol1
mount /dev/zvol/EjemploRaidZ/vol1 /home/vagrant/vol1</code></pre></figure>

<p>Comprobamos como se ha montado</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">lsblk <span class="nt">-f</span>
NAME   FSTYPE     LABEL        UUID                                 FSAVAIL FSUSE% MOUNTPOINT

...
zd0                                                                  174.2M     1% /home/vagrant/vol1</code></pre></figure>

<h3 id="cifrado">Cifrado</h3>

<p>Esta funcionalidad nos sería bastante util en casos como por ejemplo, almacenamiento de datos crítios en copias de seguridad. Para activarlo sobre una unidad de nuestro sistema de ficheros debemos ejecutar un comando con la siguiente sintaxis:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">zpool create <span class="nt">-O</span> <span class="nv">encryption</span><span class="o">=</span>off|on|distintos-algoritmos <span class="nt">-O</span> <span class="nv">keyformat</span><span class="o">=</span>passphrase <span class="se">\</span>
<span class="nt">-O</span> <span class="nv">keylocation</span><span class="o">=</span>prompt NombreDelPool /dev/DispositivoDeBloque</code></pre></figure>

<p>En mi caso voy a utilizar el volumen que creamos antes</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">zpool create <span class="nt">-O</span> <span class="nv">encryption</span><span class="o">=</span>on <span class="nt">-O</span> <span class="nv">keyformat</span><span class="o">=</span>passphrase <span class="nt">-O</span> <span class="nv">keylocation</span><span class="o">=</span>prompt <span class="se">\</span>
PoolCifrado /dev/zvol/EjemploRaidZ/vol1</code></pre></figure>

<p>Y comprobamos que se ha creado correctamente con el cifrado correspondiente</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">zfs get encryption PoolCifrado
NAME         PROPERTY    VALUE        SOURCE
PoolCifrado  encryption  aes-256-ccm  -</code></pre></figure>

<h3 id="snapshots">Snapshots</h3>

<p>El sistema de <em>snapshots</em> utilizado por <strong>ZFS</strong> permite realizar instantáneas del sistema que, se almacenan en el mismo dispositivo de bloques donde se está utilizando dicho sistema de ficheros. Esto nos impediría utilizarlas en caso de desastre, pero siempre podemos recurrir a la replicación y copiarlas en otro dispositivo separado de <strong>ZFS</strong>.
Aunque al principio estas <em>snapshots</em> no ocupan espacio, comenzarán a crecer en función de los cambios que experimente el sistema desde que se creó la instantánea.
Para crear o eliminar un <em>snapshot</em> la sintaxis es la siguiente.</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">zfs snapshot zpool/vol@nombre-de-snapshot
zfs destroy zpool/vol@nombre-de-snapshot</code></pre></figure>

<p>Y en caso de que quisiésemos volver a una <em>snapshot</em> que ya hicimos con anterioridad ejecutaríamos un comando como este:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="c"># utilizaríamos el parámetro -r en caso de que</span>
<span class="c"># quisiésemos eliminar automáticamente la snapshot</span>
<span class="c"># tras hacer el "rollback"</span>
zfs rollback <span class="nt">-r</span> zpool/vol@nombre-de-snapshot</code></pre></figure>

<h3 id="otras-funcionalidades">Otras funcionalidades</h3>

<p>Tal y como hemos visto a lo largo de esta entrada, <strong>ZFS</strong> es verdaderamente un sistema de ficheros avanzado, pero no se queda ahí. Además de todo lo que ya hemos visto (COW, RAID, Volumenes, etc), dispone de otras aplicaciones.</p>

<h4 id="discos-caché-y-log-en-raid">Discos caché y log en RAID</h4>

<p>Para un aumento del rendimient <em>ZFS</em> nos ofrece la posibilidad de añadir discos caché. Esto tendría sentido en el supuesto de tener el almacenamiento principal del <em>RAID</em> en discos mecánicos y añadir un disco de estado sólido como un <a href="https://youtu.be/ANVvxlv6DV4">M.2 NVME PCIE</a>.
También nos permite añadir especificar un disco <strong>log</strong>. Inicialmente pensaríamos que es un disco para guardar los ficheros de <em>log</em> del sistema, como los de las unidades <em>systemd</em>, aplicaciones como <em>apache2</em>, <em>nginx</em>, etc. Sin embargo, es una unidad también de tipo caché, que se encarga de hacer más liviano el <em>copy-on-write</em>, sobre todo cuando se necesitan estructuras síncronas. A este tipo de dispositivo se le conoce como <strong>ZFS Intent Log</strong> o <strong>ZIL</strong>.
Si quisiésemos definir este tipo de unidades en el <em>RAID</em>, tan solo tenemos que añadir a la derecha de la unidad en cuestión el parámetro <code class="language-plaintext highlighter-rouge">cache</code> en caso de disco caché convencional o <code class="language-plaintext highlighter-rouge">log</code> en caso de un disco orientado al <strong>ZIL</strong> que hemos explicado antes.</p>

<h2 id="conclusión">Conclusión</h2>

<p><strong>ZFS</strong> es verdaderamente un sistema de ficheros avanzado. Aporta una serie de funcionalidades que lo hacen sumamente útil en el caso de que necesitemos un sistema de ficheros <em>robusto</em>. Más allá de la <em>redundancia</em> que aporta según el tipo de <em>RAID</em> que utilicemos, hemos podido comprobar otras funciones adicionales respecto a los sistemas tradicionales como el <em>copy-on-write</em> o la posibilidad de generar dispositivos de bloques completamente funcionales, además de la creación de <em>snapshots</em>.
No obstante, debido a la complejidad de su instalación y la incompatibilidad de la licencia <strong>CDDL</strong>, incompatible con la licencia de <em>Linux</em> <strong>GPL</strong>, hace que su uso no esté tan extendido.
Dicho esto, aunque creo que el uso <strong>ZFS</strong> merece la pena debido a todo el potencial que tiene, su licencia lo condenó a caer cada vez más en el desuso, y todavía más con el desarrollo contínuo de <strong>BTRFS</strong>, el cual sí tiene una licencia compatible con el <em>Kernel Linux</em></p>

            </article>

            <section class="share">
    <h3>Compartir</h3>
    <a aria-label="Share on Twitter" href="https://twitter.com/intent/tweet?text=&quot;&quot;%20https://blog.luisvazquezalejo.es/Sistema-de-ficheros-ZFS/%20via%20&#64;&hashtags=zfs,filesystem,"
    onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;" title="Compartir en Twitter">
        <svg class="icon icon-twitter"><use xlink:href="#icon-twitter"></use></svg>
    </a>
    <a aria-label="Share on Facebook"href="https://www.facebook.com/sharer/sharer.php?u=https://blog.luisvazquezalejo.es/Sistema-de-ficheros-ZFS/"
    onclick="window.open(this.href, 'facebook-share','width=580,height=296');return false;" title="Compartir en Facebook">
        <svg class="icon icon-facebook"><use xlink:href="#icon-facebook"></use></svg>
    </a>
</section>

            <section class="author" itemprop="author">
    <div class="details" itemscope itemtype="http://schema.org/Person">
        <img itemprop="image" class="img-rounded" src="/assets/img/blog-author.jpg" alt="">
        <p class="def">Autor</p>
        <p class="desc">Administrador de Sistemas Informáticos en Red</p>
        <a itemprop="email" class="email" href="mailto:mail@luisvazquezalejo.es">mail@luisvazquezalejo.es</a>
    </div>
</section>

            <footer>
    <p>Plantilla hecha con <a href="https://jekyllrb.com/" target="_blank">Jekyll</a> y <span class="love">❤</span> por <a href="https://willianjusten.com.br">Willian Justen</a></p>
</footer>
<script src="/assets/js/main.js"></script>

        </section>
    </body>
</html>
